{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import helpers_05\n",
    "\n",
    "%matplotlib inline   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to give our model more spatial awareness, provide translation invariance, and reduce the amount of parameters in our network, we need to share weights in such a way that the same pixels located in different parts of a image output identical activation values. The technique we end up using is called a _convolution_.\n",
    "\n",
    "![](images/convolution_animated.gif)\n",
    "\n",
    "The defining feature of a convolution is the _kernel_ (also known as a _filter_), which is a grid-like set of weights which slides over regions of an input image. At each step, the kernel weights are multiplied with the corresponding pixel values underneath. These multiplied values are then summed to get the output value at that point.\n",
    "\n",
    "![](images/convolution_still.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By performing the operation in this way, we can use local spatial information while also making sure that each part of the image can be examined with the same weights.\n",
    "\n",
    "This is something that is easier to learn visually, so let's take a look at a simple example:\n",
    "\n",
    "![](images/basic_kernel_anim.gif)\n",
    "\n",
    "Here, we have a 5x5 input matrix, and our kernel is 3x3. When the kernel is placed in the top-left portion of the input, we end up with the following total sum:\n",
    "\n",
    "```\n",
    "(1 * -1) + (2 * 1) + (0 * 2) + (1 * 1) + (0 * 1) + (0 * 0) + (2 * -1)+ (2 * 0) = -2\n",
    "```\n",
    "\n",
    "We can see that the value of `-2` is the top-left entry in our output matrix. As we slide the kernel across the image, we get corresponding outputs which follow the kernel spatially. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Convolutions with NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick example in NumPy/TensorFlow to verify that the above illustration works. First, we'll create our input matrix and kernels in NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example input matrix\n",
    "a = np.array([[1, 2, 0, 3, 1],\n",
    "              [1, 0, 0, 2, 2],\n",
    "              [2, 1, 2, 1, 1],\n",
    "              [0, 0, 1, 0, 0],\n",
    "              [1, 2, 1, 1, 1]]).reshape(1,5,5,1).astype(np.float32)\n",
    "\n",
    "# Example kernel\n",
    "kernel = np.array([[-1,  1, 2],\n",
    "                   [ 1,  1, 0],\n",
    "                   [-1, -2, 0]]).reshape(3,3,1,1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the values we set in our NumPy arrays match those in the visual above. A small implementation quirk is that we reshape the NumPy arrays.  The input matrix goes from `[5, 5]` to `[1, 5, 5, 1]`. The values, from left to right, represent the number of elements:\n",
    "  * in the batch (1), \n",
    "  * the height (5), \n",
    "  * the width (5), and \n",
    "  * the number of channels (1). \n",
    "  \n",
    "If our test matrix represented an RGB image, the shape would be `[1, 5, 5, 3]`. If there were 10 images in the batch, the shape would be `[10, 5, 5, 1]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel goes from `[3, 3]` to `[3, 3, 1, 1]`. The values, from left to right, represent:\n",
    "  * the height of the kernel (3), \n",
    "  * the width of the kernel (3), \n",
    "  *  the number of channels in the _input_ (1), and \n",
    "  *  the number of channels in the output (1). \n",
    "  \n",
    "If the input represented an RGB image, we'd have the shape `[3, 3, 3, 1]`, and if we wanted our output to have 5 channels, the output shape would be `[3, 3, 1, 5]`.  Think of the last two components as being similar to a weight matrix on a fully connected layer, which has the shape `[prev_num_neurons, curr_num_neurons]`.\n",
    "    \n",
    "Now that we have our dummy data, let's run it and see what we get!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2. -1.  3.]\n",
      " [ 2.  5.  8.]\n",
      " [-2.  0. -1.]]\n"
     ]
    }
   ],
   "source": [
    "out = tf.Session().run(tf.nn.conv2d(input=a,\n",
    "                                    filter=kernel,\n",
    "                                    strides=[1, 1, 1, 1],\n",
    "                                    padding='VALID'))\n",
    "# np.squeeze() removes dimensions equal to `1` from a matrix/tensor\n",
    "# The result of tf.conv2d is four dimensional, so this cleans it up\n",
    "print(out.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf.nn.conv2d()` Operation expects four main arguments: `input`, `filter`, `strides`, and `padding`:\n",
    "\n",
    "* `input` is the input Tensor, in this case our `[1, 5, 5, 1]` test data.\n",
    "* `filter` is the kernel. Typically, this would be a `tf.Variable`, but in this demo it's just our `[3,3,1,1]` kernel.\n",
    "\n",
    "The other two inputs are things we haven't talked about yet, stride and padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stride\n",
    "\n",
    "\"Stride\" refers to the number of squares we move our kernel for each step (both vertically and horizontally). In the above example, both our strides (horizontal and vertical) were 1, but if they were set to 2, the operation would look like this:\n",
    "\n",
    "![](images/basic_kernel_stride2_anim.gif)\n",
    "\n",
    "One note:  we \"walk\" across using the horizontal stride (across columns) before we \"reset\" back to the left with the vertical stride (down rows).  This is almost like an old-school typewriter with a *ding* when you get to the end of the line.\n",
    "\n",
    "Because we're skipping over a square in both directions, our output `Tensor` is 2x2 instead of 3x3. This is one way to reduce the spatial dimensions of a network- we'll look at another technique for this, pooling, later in this lesson.\n",
    "\n",
    "The way we set the stride in TensorFlow is with the `stride` parameter, which is a list of integers with the form `[1, vert_stride, hori_stride, 1]` (the first and last elements have to be `1`. We can test this out by modifying the code from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.  3.]\n",
      " [-2. -1.]]\n"
     ]
    }
   ],
   "source": [
    "out = tf.Session().run(tf.nn.conv2d(input=a,\n",
    "                                    filter=kernel,\n",
    "                                    strides=[1, 2, 2, 1],\n",
    "                                    padding='VALID'))\n",
    "print(out.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Padding\n",
    "\n",
    "You may have noticed that we ended up reducing the spatial dimensions from our input to the output: we start with a `5x5` Tensor, and end up with a `3x3` Tensor as output. While this is sometimes ok, it's often the case that we want to maintain the width and height of out input. We can achieve this by adding zeros around our image or *padding* the image. By adding a ring of one or more zeros to the outside of our input and letting our kernel \"overflow\" on the sides, we can ensure that the output dimensions match the input dimensions.\n",
    "\n",
    "![](images/padded_kernel_anim.gif)\n",
    "\n",
    "_Side note: the amount of zeros needed on the outside of the input depends on the size of the kernel: `3x3` needs one layer of zeros, `5x5` needs two, `7x7` needs three, etc._\n",
    "\n",
    "We can control padding in TensorFlow by using the `padding` parameter. `padding` takes a string which selects between two options:\n",
    "\n",
    "* `'VALID'` is what we used above - no zero padding. The idea is that we're only using \"real\" or \"valid\" input data to get outputs. You could also think of it as the kernel not going \"out of bounds\", thus staying \"valid\"\n",
    "* `'SAME'` uses zero-padding to keep the output dimensions equal to that of the input (assuming horizontal and vertical stride is set to 1). This one is more self-explanatory: we're keeping the dimensions the \"same\"\n",
    "\n",
    "_Side note: this naming scheme comes from the convolution syntax in [NumPy](https://docs.scipy.org/doc/numpy/reference/generated/numpy.convolve.html) and [MATLAB](https://www.mathworks.com/help/matlab/ref/conv2.html)_\n",
    "\n",
    "Let's try out `'SAME'` padding with our dummy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.  2.  2. -1. -2.]\n",
      " [ 2. -2. -1.  3. -1.]\n",
      " [ 3.  2.  5.  8.  2.]\n",
      " [ 2. -2.  0. -1. -3.]\n",
      " [ 1.  5.  4.  1.  2.]]\n"
     ]
    }
   ],
   "source": [
    "out = tf.Session().run(tf.nn.conv2d(input=a,\n",
    "                                    filter=kernel,\n",
    "                                    strides=[1, 1, 1, 1],\n",
    "                                    padding='SAME'))\n",
    "print(out.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple layers of input and kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/two_layer_kernel_anim.gif)\n",
    "\n",
    "![](images/two_layer_kernel_final.png)\n",
    "\n",
    "##### Adding a Bias\n",
    "\n",
    "We will also add in a bias term for convolutions the same way we would a fully-connected layer. Each layer of the filter will have a constant number that is added along with the rest of the kernel dot product. We create a `Variable` vector with the same depth as the output `Tensor` (the last number in the shape of the kernel). Then, we use [`tf.nn.bias_add()`](https://www.tensorflow.org/versions/master/api_docs/python/nn/activation_functions_#bias_add) to add the value the output of our convolution.\n",
    "\n",
    "```\n",
    "conv = tf.nn.conv2d(input, kernel, ...)\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[depth]))\n",
    "total = tf.nn.bias(conv, bias)\n",
    "```\n",
    "\n",
    "For convolutions, the bias is usually initialized to some small positive value (as opposed to zero). We do this because the general go-to activation function for CNNs is ReLU, which can \"die\" if they never get positive inputs.\n",
    "\n",
    "##### Small Aside About Kernels\n",
    "\n",
    "[Kernels](https://en.wikipedia.org/wiki/Kernel_(image_processing) have been used in image processing for many years for tasks such as edge detection, blurring, and sharpening. The basic idea of convolutional neural networks is that we can let the computer find useful kernels for the task instead of using handmade, pre-defined ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier, we saw that using a higher stride convolution results in collapsing the spatial dimensions of our network. While it is sometimes advantageous to do this during trained convolutions, a more common technique is to use _pooling_, which does a transformation on neighboring inputs (but is not trained/has zero parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In max pooling, the network looks at a chunk of neighboring pixels and outputs the largest pixel from that group:\n",
    "\n",
    "![Max pooling, size 2, stride 2](images/maxpool.png)\n",
    "\n",
    "In this example, we've split a `4x4` matrix into four groups (shown by  colors), each holding four numbers. We take the largest number from each group to output the matrix on the right. To demonstrate this in TensorFlow, we'll use the [`tf.nn.max_pool()`](https://www.tensorflow.org/versions/master/api_docs/python/nn/pooling#max_pool) Op. Because each group is a `[2x2]` box, and there are 2 steps between boxes (horizontally and vertically), we say that this is a max pool, size 2, stride 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8. 5.]\n",
      " [1. 4.]]\n"
     ]
    }
   ],
   "source": [
    "# Setup our input array\n",
    "pool_me = np.array([[ 2,  1,  0, -1],\n",
    "                    [-3,  8,  2,  5],\n",
    "                    [ 1, -1,  3,  4],\n",
    "                    [ 0,  1,  1, -2]]).reshape(1,4,4,1).astype(np.float32)\n",
    "\n",
    "out = tf.Session().run(tf.nn.max_pool(value=pool_me,\n",
    "                                      ksize=[1, 2, 2, 1],\n",
    "                                      strides=[1, 2, 2, 1],\n",
    "                                      padding='VALID'))\n",
    "print(out.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that max pooling has similar parameters to `tf.nn.conv2d()`. The primary difference is `ksize` instead of `filter`: `ksize` is a list of integers representing the shape of the pooling kernel with shape `[1, k_height, k_width, 1]`. In this case, it's `[1, 2, 2, 1]`, as our kernel is two squares high and two squares wide.\n",
    "\n",
    "`stride` and `padding` act the same as they do for convolutions. In general the `strides` option is the same or larger as `ksize` (no overlapping kernels), and `padding` is generally set to `'VALID'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average pooling is the same idea as max pooling, only instead we take the average of all the values in each group.\n",
    "\n",
    "![Average pooling, size 2, stride 2](images/avgpool.png)\n",
    "\n",
    "We can use average pooling with the [`tf.nn.avg_pool`](https://www.tensorflow.org/versions/master/api_docs/python/nn/pooling#avg_pool) Op:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.   1.5 ]\n",
      " [0.25 1.5 ]]\n"
     ]
    }
   ],
   "source": [
    "out = tf.Session().run(tf.nn.avg_pool(value=pool_me,\n",
    "                                      ksize=[1, 2, 2, 1],\n",
    "                                      strides=[1, 2, 2, 1],\n",
    "                                      padding='VALID'))\n",
    "print(out.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max or Avg Pool?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The jury is still out as to whether or not there is a definitive answer whether max pooling or average pooling is more effective. At this time, I'd say max pooling is more common, though average pooling is used in several well-regarded modern models.\n",
    "\n",
    "You can read a thorough discussion in [_A Theoretical Analysis of Feature Pooling in Visual Recognition_ (Boureau et al)](http://people.ee.duke.edu/~lcarin/icml2010b.pdf) if you want to dive deeper into the debate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[FIXME: Add text.]  Our goal now is to implement LeNet.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we're going to make use a few names that we defined last week\n",
    "# they've been gathered in helpers_05.py\n",
    "from helpers_05 import (batches, flatten, fully_connected_layer,\n",
    "                        fully_connected_sigmoid_layer,\n",
    "                        test_and_show_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[FIXME:  add text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(incoming, num_kernels, kernel_sz, \n",
    "               strides=[1, 1, 1, 1], padding='SAME',\n",
    "               bval=0.01, \n",
    "               activation_fn=tf.nn.relu, \n",
    "               name=None):\n",
    "    prev_outshape = incoming.shape.dims[-1].value\n",
    "    kshape = kernel_sz + [prev_outshape, num_kernels]\n",
    "\n",
    "    fan_in = np.prod(incoming.shape[1:]).value\n",
    "    xavier_stddev = np.sqrt(2.0 / fan_in)\n",
    "    \n",
    "    with tf.variable_scope(name, 'conv_layer'):\n",
    "        w = tf.Variable(tf.truncated_normal(kshape, stddev=xavier_stddev), name='kernel')\n",
    "        b = tf.Variable(tf.constant(bval, shape=[num_kernels]), name='bias')\n",
    "        conv = tf.nn.conv2d(incoming, w, strides, padding, name='conv')\n",
    "        z = tf.nn.bias_add(conv, b)\n",
    "        return z if activation_fn is None else activation_fn(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool_layer(incoming, ksize, strides=None, padding='VALID',\n",
    "               pool_fn=tf.nn.max_pool, name=None):\n",
    "    'create a pooling layer:  we auto-add the leading/trailing 1s'\n",
    "    ksize = [1] + ksize + [1]\n",
    "    # default strides to ksize\n",
    "    strides = strides if strides is not None else ksize\n",
    "    with tf.variable_scope(name, 'pool_layer'):\n",
    "        return pool_fn(incoming, ksize, strides, padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet Sub-Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get us warmed up, here is a simple network that shares some of the architecture of LeNet, but it is smaller and it uses sigmoid activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def not_lenet(images):\n",
    "    with tf.name_scope('small_model'):\n",
    "        conv_1 = conv_layer(images, 6, [5, 5])\n",
    "        print(tf.Session().run(conv_1).shape)\n",
    "        pool_1 = pool_layer(conv_1, [2, 2])\n",
    "        print(tf.Session().run(pool_1).shape)\n",
    "        conv_2 = conv_layer(pool_1, 16, [5, 5])\n",
    "        print(tf.Session().run(conv_2).shape)\n",
    "        pool_2 = pool_layer(conv_2, [2, 2])\n",
    "        print(tf.Session().run(pool_2).shape)\n",
    "        \n",
    "        flat = flatten(pool_2)\n",
    "        \n",
    "        fc_layer = fully_connected_sigmoid_layer\n",
    "        fc_1 = fc_layer(flat, 120)\n",
    "        fc_2 = fc_layer(fc_1,  84)\n",
    "        fc_3 = fully_connected_layer(fc_2,  10, w_stddev = 0.5, activation_fn=None)\n",
    "    \n",
    "    return fc_3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv_layer_2/Relu:0\", shape=(1, 4, 4, 6), dtype=float32)\n",
      "Tensor(\"pool_layer_2/MaxPool:0\", shape=(1, 2, 2, 6), dtype=float32)\n",
      "Tensor(\"conv_layer_3/Relu:0\", shape=(1, 2, 2, 16), dtype=float32)\n",
      "Tensor(\"pool_layer_3/MaxPool:0\", shape=(1, 1, 1, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "my_img = np.array([[ [2,0,0],  [1,0,0],  [0,0,0], [-1,0,0]],\n",
    "                    [[-3,0,0],  [8,0,0],  [2,0,0],  [5,0,0]],\n",
    "                    [ [1,0,0], [-1,0,0],  [3,0,0],  [4,0,0]],\n",
    "                    [ [0,0,0],  [1,0,0],  [1,0,0], [-2,0,0]]]).reshape(1,4,4,3).astype(np.float32)\n",
    "conv_1 = conv_layer(tf.constant(my_img), 6, [2, 2])\n",
    "pool_1 = pool_layer(conv_1, [2, 2])\n",
    "conv_2 = conv_layer(pool_1, 16, [2, 2])\n",
    "pool_2 = pool_layer(conv_2,[2,2])\n",
    "print(conv_1)\n",
    "print(pool_1)\n",
    "print(conv_2)\n",
    "print(pool_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We very briefly discussed Xavier initialization in the slides.  It isn't that hard to implement, either.  Here, we simply add a value to compute `w_stddev` instead of setting it to a fixed constant (see line 8 in the next cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected_xavier_relu_layer(incoming_layer, num_nodes,\n",
    "                                      b_val=0.01,\n",
    "                                      keep_prob=None, name=None):\n",
    "    ' pass through for fully_connected_layer with xavier init '\n",
    "    incoming_layer = tf.convert_to_tensor(incoming_layer)\n",
    "    prev_num_nodes = incoming_layer.shape.dims[-1].value\n",
    "    \n",
    "    w_stddev = np.sqrt(2.0 / prev_num_nodes)\n",
    "\n",
    "    return fully_connected_layer(incoming_layer, num_nodes,\n",
    "                                 w_stddev = w_stddev, b_val=b_val,\n",
    "                                 activation_fn = tf.nn.relu,\n",
    "                                 keep_prob=keep_prob,\n",
    "                                 name=name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above as a starting point, we can now recrete that actual LeNet.  Here's the table from the slides:\n",
    "\n",
    "![](images/lenet-table.png)\n",
    "\n",
    "and the graphic from the paper:\n",
    "\n",
    "![](images/lenet.png)\n",
    "\n",
    "Unfortunately, you have to combine information from the table and the graphic to get the numbers below.  The depths come from the graphic (they precede the `@` signs on the top of the figure).  The other values come from the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lenet_small(images):\n",
    "    with tf.name_scope('small_lenet_model'):    \n",
    "        conv_1 = conv_layer(images, 6, [5, 5])\n",
    "        print()\n",
    "        pool_1 = pool_layer(conv_1, [2, 2])\n",
    "        conv_2 = conv_layer(pool_1, 16, [5, 5])\n",
    "        pool_2 = pool_layer(conv_2, [2, 2])\n",
    "\n",
    "        flat = flatten(pool_2)\n",
    "\n",
    "        fc_layer = fully_connected_xavier_relu_layer\n",
    "        fc_1   = fc_layer(flat, 120)\n",
    "        fc_2   = fc_layer(fc_1, 84)\n",
    "        fc_3   = fc_layer(fc_2, 10)\n",
    "        fc_4   = fully_connected_layer(fc_3, 10, activation_fn=None)\n",
    "    \n",
    "    return fc_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's a scaled up LeNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lenet_big(images):\n",
    "    with tf.name_scope('big_lenet_model'):    \n",
    "        conv_1 = conv_layer(images, 32, [5, 5])\n",
    "        pool_1 = pool_layer(conv_1, [2, 2])\n",
    "        conv_2 = conv_layer(pool_1, 64, [5, 5])\n",
    "        pool_2 = pool_layer(conv_2, [2, 2])\n",
    "\n",
    "        flat = flatten(pool_2)\n",
    "\n",
    "        fc_layer = fully_connected_xavier_relu_layer\n",
    "        fc_1   = fc_layer(flat, 400)\n",
    "        fc_2   = fc_layer(fc_1, 200)\n",
    "        fc_3   = fc_layer(fc_2, 200)\n",
    "        fc_4   = fully_connected_layer(fc_3, 10, activation_fn=None)\n",
    "    \n",
    "    return fc_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You mission, should you choose to accept it, is to take the pieces we've developed and turn them into a working model for Lenet on MNIST.  Here's a process to get you started:\n",
    "\n",
    "  1. Grab either your CIFAR or MNIST model from last week.\n",
    "  2.  Modify it so that your s.out is coming from one of the LeNet versions we defined above.\n",
    "  3.  Modify your `training` block so it uses a slightly different optimizer (we will talk about these in the coming weeks).\n",
    "```\n",
    "    with tf.name_scope('train'):\n",
    "        decayed_rate = tf.train.exponential_decay(s.learning_rate, global_step,\n",
    "                                              600, 0.998, True)\n",
    "        momopt = tf.train.MomentumOptimizer\n",
    "        s.train = momopt(decayed_rate, 0.9).minimize(s.loss)\n",
    "```\n",
    "  4.  Read in your training and testing data with `helpers_05.get_mnist_dataset` (like we did last week)\n",
    "  5.  Setup a training loop like we did last week.  Note, use a `learning_rate=0.001` and we don't need a `momentum` (which we used last time).\n",
    "  6.  If things are working right, you shouldn't need 15 epochs of training.  Five or six should do.\n",
    "  7.  Good luck!  Feel free to consult the solution below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(train_data,train_labels,test_data,test_labels)=helpers_05.get_mnist_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape\n",
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MNIST_Model():\n",
    "    def __init__(s):\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            with tf.name_scope(\"input\"):\n",
    "                s.x = tf.placeholder(tf.uint8,shape=[None,28,28,1],name='x')\n",
    "                s.y = tf.placeholder(tf.int32,shape=[None],name='y')\n",
    "            with tf.name_scope(\"hyperparams\"):\n",
    "                s.learning_rate = tf.placeholder(tf.float32,[],name='learning_rate')\n",
    "                s.x_keep_prob = tf.placeholder(tf.float32,[],name='x_keep_prob')\n",
    "                s.h_keep_prob = tf.placeholder(tf.float32,[],name='h_keep_prob')\n",
    "            with tf.name_scope(\"preprocess\"):\n",
    "                s.x_float = tf.cast(s.x,tf.float32)\n",
    "                s.x_dropped = tf.nn.dropout(s.x_float,keep_prob=s.x_keep_prob)\n",
    "                s.one_hot_lables = tf.one_hot(s.y,10)\n",
    "            with tf.name_scope(\"model\"):\n",
    "                s.conv1 = conv_layer(s.x_dropped,6,[5,5])\n",
    "                s.pool1 = pool_layer(s.conv1,[2,2])\n",
    "                s.conv2 = conv_layer(s.pool1,16,[5,5])\n",
    "                s.pool2 = pool_layer(s.conv2,[2,2])\n",
    "                flat = flatten(s.pool2)\n",
    "                fc_layer = fully_connected_xavier_relu_layer\n",
    "                s.fc1 = fc_layer(flat, 120,keep_prob=s.h_keep_prob)\n",
    "                s.fc2 = fc_layer(s.fc1, 84,keep_prob=s.h_keep_prob)\n",
    "                s.out = fc_layer(s.fc2, 10,keep_prob=s.h_keep_prob)\n",
    "                #s.out = fully_connected_layer(s.fc3, 10, activation_fn=None)\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                smce = tf.nn.softmax_cross_entropy_with_logits_v2\n",
    "                s.loss = tf.reduce_mean(smce(logits = s.out,labels = s.one_hot_lables))\n",
    "            with tf.name_scope(\"global_step\"):\n",
    "                global_step = tf.Variable(0,trainable=False,name=\"global_step\" )\n",
    "                s.inc_step = tf.assign(global_step,global_step+1,name=\"inc_step\")\n",
    "            with tf.name_scope(\"train\"):\n",
    "                decayed_rate = tf.train.exponential_decay(s.learning_rate, global_step,\n",
    "                                        600, 0.998, True)\n",
    "                momopt = tf.train.MomentumOptimizer(decayed_rate,0.9)\n",
    "                s.train = momopt.minimize(s.loss)\n",
    "            with tf.name_scope(\"prediction\"):\n",
    "                s.softmax = tf.nn.softmax(s.out,name=\"softmax\")\n",
    "                s.prediction = tf.cast(tf.argmax(s.softmax,1),tf.int32)\n",
    "                s.correct = tf.equal(s.prediction,s.y)\n",
    "                s.accuracy = tf.reduce_mean(tf.cast(s.correct,tf.float32)) \n",
    "            init = tf.global_variables_initializer()\n",
    "        s.sess = tf.Session(graph=graph)\n",
    "        s.sess.run(init)\n",
    "    \n",
    "    def fit(s,train_dict):\n",
    "        tr_loss,step,tr_acc,_ = s.sess.run([s.loss,s.inc_step,s.accuracy,s.train],feed_dict=train_dict)\n",
    "        return tr_loss,step,tr_acc,_\n",
    "    \n",
    "    def predict(s,test_dict):\n",
    "        ct_correct,pred = s.sess.run([s.correct,s.prediction],feed_dict=test_dict)\n",
    "        return ct_correct,pred\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 step: 600 loss: 0.7272697687149048 acc: 0.7400000095367432\n",
      "epoch: 1 step: 1200 loss: 0.6253323554992676 acc: 0.75\n",
      "epoch: 2 step: 1800 loss: 0.8116852045059204 acc: 0.6899999976158142\n",
      "epoch: 3 step: 2400 loss: 0.5320529341697693 acc: 0.800000011920929\n",
      "epoch: 4 step: 3000 loss: 0.16889211535453796 acc: 0.9399999976158142\n",
      "epoch: 5 step: 3600 loss: 0.05312144383788109 acc: 0.9900000095367432\n",
      "epoch: 6 step: 4200 loss: 0.13868233561515808 acc: 0.9599999785423279\n",
      "epoch: 7 step: 4800 loss: 0.0440414696931839 acc: 0.9800000190734863\n",
      "epoch: 8 step: 5400 loss: 0.05750611424446106 acc: 0.9900000095367432\n",
      "epoch: 9 step: 6000 loss: 0.042615074664354324 acc: 0.9700000286102295\n"
     ]
    }
   ],
   "source": [
    "CNN_mm = MNIST_Model()\n",
    "\n",
    "for epoch in range(10):\n",
    "    for batch_data,batch_labels in batches(train_data,train_labels,100):\n",
    "        train_dict ={CNN_mm.x:batch_data,\n",
    "                     CNN_mm.y:batch_labels,\n",
    "                     CNN_mm.learning_rate:0.001,\n",
    "                     CNN_mm.x_keep_prob:0.8,\n",
    "                     CNN_mm.h_keep_prob:1\n",
    "        }\n",
    "        tr_loss,step,tr_acc,_ = CNN_mm.fit(train_dict)\n",
    "    print(f\"epoch: {epoch} step: {step} loss: {tr_loss} acc: {tr_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9779\n"
     ]
    }
   ],
   "source": [
    "#get test acc\n",
    "batch_correct_cts = []\n",
    "for batch_data, batch_labels in batches(test_data, test_labels, 200):\n",
    "    test_dict = {CNN_mm.x : batch_data,   CNN_mm.x_keep_prob : 1.0,\n",
    "                 CNN_mm.y : batch_labels, CNN_mm.h_keep_prob : 1.0}\n",
    "    correctness, curr_preds = CNN_mm.predict(test_dict)\n",
    "\n",
    "    batch_correct_cts.append(correctness.sum())\n",
    "\n",
    "print(sum(batch_correct_cts) / len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got a more powerful network architecture, let's try to do better on CIFAR.  Take your new and improved LeNet model and point it at the CIFAR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempting to download cifar\n",
      "Extracting data for data/cifar\\cifar-10-python. This may take a while. Please wait.\n"
     ]
    }
   ],
   "source": [
    "import helpers_04\n",
    "(train_data, train_labels,\n",
    "            test_data, test_labels), readable_labels = helpers_04.load_cifar()\n",
    "#when you run the code you may encounter errors,just copy the floder \"data/cifar-10-batches-py\" to folder \"data/cifar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data,test_data=train_data.reshape((-1,32,32,3)),test_data.reshape((-1,32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CIFAR_Model():\n",
    "    def __init__(s):\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            with tf.name_scope(\"input\"):\n",
    "                s.x = tf.placeholder(tf.uint8,shape=[None,32,32,3],name='x')\n",
    "                s.y = tf.placeholder(tf.int32,shape=[None],name='y')\n",
    "            with tf.name_scope(\"hyperparams\"):\n",
    "                s.learning_rate = tf.placeholder(tf.float32,[],name='learning_rate')\n",
    "                s.x_keep_prob = tf.placeholder(tf.float32,[],name='x_keep_prob')\n",
    "                s.h_keep_prob = tf.placeholder(tf.float32,[],name='h_keep_prob')\n",
    "            with tf.name_scope(\"preprocess\"):\n",
    "                s.x_float = tf.cast(s.x,tf.float32)\n",
    "                s.x_dropped = tf.nn.dropout(s.x_float,keep_prob=s.x_keep_prob)\n",
    "                s.one_hot_lables = tf.one_hot(s.y,10)\n",
    "            with tf.name_scope(\"model\"):\n",
    "                s.conv1 = conv_layer(s.x_dropped,32,[5,5])\n",
    "                s.pool1 = pool_layer(s.conv1,[2,2])\n",
    "                s.conv2 = conv_layer(s.pool1,64,[5,5])\n",
    "                s.pool2 = pool_layer(s.conv2,[2,2])\n",
    "                flat = flatten(s.pool2)\n",
    "                fc_layer = fully_connected_xavier_relu_layer\n",
    "                s.fc1 = fc_layer(flat, 600,keep_prob=s.h_keep_prob)\n",
    "                s.fc2 = fc_layer(s.fc1, 300,keep_prob=s.h_keep_prob)\n",
    "                s.fc3 = fc_layer(s.fc2, 300,keep_prob=s.h_keep_prob)\n",
    "                s.out = fully_connected_layer(s.fc3, 10, activation_fn=None)\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                smce = tf.nn.softmax_cross_entropy_with_logits_v2\n",
    "                s.loss = tf.reduce_mean(smce(logits = s.out,labels = s.one_hot_lables))\n",
    "            with tf.name_scope(\"global_step\"):\n",
    "                global_step = tf.Variable(0,trainable=False,name=\"global_step\" )\n",
    "                s.inc_step = tf.assign(global_step,global_step+1,name=\"inc_step\")\n",
    "            with tf.name_scope(\"train\"):\n",
    "                decayed_rate = tf.train.exponential_decay(s.learning_rate, global_step,\n",
    "                                        600, 0.998, True)\n",
    "                momopt = tf.train.MomentumOptimizer(decayed_rate,0.9)\n",
    "                s.train = momopt.minimize(s.loss)\n",
    "            with tf.name_scope(\"prediction\"):\n",
    "                s.softmax = tf.nn.softmax(s.out,name=\"softmax\")\n",
    "                s.prediction = tf.cast(tf.argmax(s.softmax,1),tf.int32)\n",
    "                s.correct = tf.equal(s.prediction,s.y)\n",
    "                s.accuracy = tf.reduce_mean(tf.cast(s.correct,tf.float32)) \n",
    "            init = tf.global_variables_initializer()\n",
    "        s.sess = tf.Session(graph=graph)\n",
    "        s.sess.run(init)\n",
    "    \n",
    "    def fit(s,train_dict):\n",
    "        tr_loss,step,tr_acc,_ = s.sess.run([s.loss,s.inc_step,s.accuracy,s.train],feed_dict=train_dict)\n",
    "        return tr_loss,step,tr_acc,_\n",
    "    \n",
    "    def predict(s,test_dict):\n",
    "        ct_correct,pred = s.sess.run([s.correct,s.prediction],feed_dict=test_dict)\n",
    "        return ct_correct,pred\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 step: 500 loss: 2.3004088401794434 acc: 0.10999999940395355\n",
      "epoch: 1 step: 1000 loss: 2.3015239238739014 acc: 0.1599999964237213\n",
      "epoch: 2 step: 1500 loss: 2.305943727493286 acc: 0.07000000029802322\n",
      "epoch: 3 step: 2000 loss: 2.293351650238037 acc: 0.14000000059604645\n",
      "epoch: 4 step: 2500 loss: 2.1881067752838135 acc: 0.23999999463558197\n",
      "epoch: 5 step: 3000 loss: 1.6883143186569214 acc: 0.44999998807907104\n",
      "epoch: 6 step: 3500 loss: 1.5064512491226196 acc: 0.44999998807907104\n",
      "epoch: 7 step: 4000 loss: 1.7089828252792358 acc: 0.4699999988079071\n",
      "epoch: 8 step: 4500 loss: 1.6168855428695679 acc: 0.4099999964237213\n",
      "epoch: 9 step: 5000 loss: 1.3416905403137207 acc: 0.5400000214576721\n",
      "epoch: 10 step: 5500 loss: 1.301720380783081 acc: 0.550000011920929\n",
      "epoch: 11 step: 6000 loss: 1.2371163368225098 acc: 0.5\n",
      "epoch: 12 step: 6500 loss: 1.0548182725906372 acc: 0.6499999761581421\n",
      "epoch: 13 step: 7000 loss: 1.1199522018432617 acc: 0.6100000143051147\n",
      "epoch: 14 step: 7500 loss: 1.1246387958526611 acc: 0.5600000023841858\n",
      "epoch: 15 step: 8000 loss: 0.8982959985733032 acc: 0.6399999856948853\n",
      "epoch: 16 step: 8500 loss: 1.1592944860458374 acc: 0.5899999737739563\n",
      "epoch: 17 step: 9000 loss: 0.9369139075279236 acc: 0.6899999976158142\n",
      "epoch: 18 step: 9500 loss: 0.8041641116142273 acc: 0.7300000190734863\n",
      "epoch: 19 step: 10000 loss: 1.0193729400634766 acc: 0.6499999761581421\n",
      "epoch: 20 step: 10500 loss: 0.6217556595802307 acc: 0.7900000214576721\n",
      "epoch: 21 step: 11000 loss: 0.4508803188800812 acc: 0.8199999928474426\n",
      "epoch: 22 step: 11500 loss: 0.5866199135780334 acc: 0.800000011920929\n",
      "epoch: 23 step: 12000 loss: 0.43593156337738037 acc: 0.8299999833106995\n",
      "epoch: 24 step: 12500 loss: 0.21430116891860962 acc: 0.9300000071525574\n",
      "epoch: 25 step: 13000 loss: 0.29766950011253357 acc: 0.8799999952316284\n",
      "epoch: 26 step: 13500 loss: 0.29474586248397827 acc: 0.8999999761581421\n",
      "epoch: 27 step: 14000 loss: 0.2281550168991089 acc: 0.9200000166893005\n",
      "epoch: 28 step: 14500 loss: 0.4969143271446228 acc: 0.8600000143051147\n",
      "epoch: 29 step: 15000 loss: 0.15880176424980164 acc: 0.9700000286102295\n",
      "epoch: 30 step: 15500 loss: 0.09764653444290161 acc: 0.9599999785423279\n",
      "epoch: 31 step: 16000 loss: 0.205030158162117 acc: 0.9399999976158142\n",
      "epoch: 32 step: 16500 loss: 0.1250329166650772 acc: 0.949999988079071\n",
      "epoch: 33 step: 17000 loss: 0.08700258284807205 acc: 0.9800000190734863\n",
      "epoch: 34 step: 17500 loss: 0.17538315057754517 acc: 0.9599999785423279\n",
      "epoch: 35 step: 18000 loss: 0.08229216933250427 acc: 0.9700000286102295\n",
      "epoch: 36 step: 18500 loss: 0.07010090351104736 acc: 0.9700000286102295\n",
      "epoch: 37 step: 19000 loss: 0.15099264681339264 acc: 0.9599999785423279\n",
      "epoch: 38 step: 19500 loss: 0.14723724126815796 acc: 0.949999988079071\n",
      "epoch: 39 step: 20000 loss: 0.2943522334098816 acc: 0.9599999785423279\n",
      "epoch: 40 step: 20500 loss: 0.05487242713570595 acc: 0.9900000095367432\n",
      "epoch: 41 step: 21000 loss: 0.09487497061491013 acc: 0.9599999785423279\n",
      "epoch: 42 step: 21500 loss: 0.09549450129270554 acc: 0.9700000286102295\n",
      "epoch: 43 step: 22000 loss: 0.13202151656150818 acc: 0.9399999976158142\n",
      "epoch: 44 step: 22500 loss: 0.0789741799235344 acc: 0.9800000190734863\n",
      "epoch: 45 step: 23000 loss: 0.0604608878493309 acc: 0.9700000286102295\n",
      "epoch: 46 step: 23500 loss: 0.030867846682667732 acc: 0.9900000095367432\n",
      "epoch: 47 step: 24000 loss: 0.03108229674398899 acc: 0.9900000095367432\n",
      "epoch: 48 step: 24500 loss: 0.0287291519343853 acc: 0.9800000190734863\n",
      "epoch: 49 step: 25000 loss: 0.04719667509198189 acc: 0.9900000095367432\n",
      "epoch: 50 step: 25500 loss: 0.02948751300573349 acc: 0.9900000095367432\n",
      "epoch: 51 step: 26000 loss: 0.12329891324043274 acc: 0.9700000286102295\n",
      "epoch: 52 step: 26500 loss: 0.048298973590135574 acc: 0.9800000190734863\n",
      "epoch: 53 step: 27000 loss: 0.04497871920466423 acc: 0.9800000190734863\n",
      "epoch: 54 step: 27500 loss: 0.01677170768380165 acc: 0.9900000095367432\n",
      "epoch: 55 step: 28000 loss: 0.028011061251163483 acc: 0.9900000095367432\n",
      "epoch: 56 step: 28500 loss: 0.09658274799585342 acc: 0.9800000190734863\n",
      "epoch: 57 step: 29000 loss: 0.00602194108068943 acc: 1.0\n",
      "epoch: 58 step: 29500 loss: 0.039015986025333405 acc: 0.9900000095367432\n",
      "epoch: 59 step: 30000 loss: 0.017278091982007027 acc: 0.9900000095367432\n"
     ]
    }
   ],
   "source": [
    "CNN_cm = CIFAR_Model()\n",
    "\n",
    "for epoch in range(60):\n",
    "    for batch_data,batch_labels in batches(train_data,train_labels,100):\n",
    "        train_dict ={CNN_cm.x:batch_data,\n",
    "                     CNN_cm.y:batch_labels,\n",
    "                     CNN_cm.learning_rate:0.001,\n",
    "                     CNN_cm.x_keep_prob:1,\n",
    "                     CNN_cm.h_keep_prob:1\n",
    "        }\n",
    "        tr_loss,step,tr_acc,_ = CNN_cm.fit(train_dict)\n",
    "    print(f\"epoch: {epoch} step: {step} loss: {tr_loss} acc: {tr_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4764\n"
     ]
    }
   ],
   "source": [
    "#get test acc\n",
    "batch_correct_cts = []\n",
    "for batch_data, batch_labels in batches(test_data, test_labels, 200):\n",
    "    test_dict = {CNN_cm.x : batch_data,   CNN_cm.x_keep_prob : 1.0,\n",
    "                 CNN_cm.y : batch_labels, CNN_cm.h_keep_prob : 1.0}\n",
    "    correctness, curr_preds = CNN_cm.predict(test_dict)\n",
    "\n",
    "    batch_correct_cts.append(correctness.sum())\n",
    "\n",
    "print(sum(batch_correct_cts) / len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "427px",
    "left": "45px",
    "top": "92.3333px",
    "width": "212px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
